{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_key = ''\n",
    "cog_endpoint = ''\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d12686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_caption(image_path, description):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    # Display the image\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    img = Image.open(image_path)\n",
    "    caption_text = ''\n",
    "    if (len(description.captions) == 0):\n",
    "        caption_text = 'No caption detected'\n",
    "    else:\n",
    "        for caption in description.captions:\n",
    "            caption_text = caption_text + \" '{}'\\n(Confidence: {:.2f}%)\".format(caption.text, caption.confidence * 100)\n",
    "    plt.title(caption_text)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_analysis(image_path, analysis):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image, ImageDraw\n",
    "    import numpy as np\n",
    "\n",
    "    # Display the image\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    a = fig.add_subplot(1,2,1)\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Get the caption\n",
    "    caption_text = ''\n",
    "    if (len(analysis.description.captions) == 0):\n",
    "        caption_text = 'No caption detected'\n",
    "    else:\n",
    "        for caption in analysis.description.captions:\n",
    "            caption_text = caption_text + \" '{}'\\n(Confidence: {:.2f}%)\".format(caption.text, caption.confidence * 100)\n",
    "    plt.title(caption_text)\n",
    "\n",
    "    # Get objects\n",
    "    if analysis.objects:\n",
    "        # Draw a rectangle around each object\n",
    "        for object in analysis.objects:\n",
    "            r = object.rectangle\n",
    "            bounding_box = ((r.x, r.y), (r.x + r.w, r.y + r.h))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            draw.rectangle(bounding_box, outline='magenta', width=5)\n",
    "            plt.annotate(object.object_property,(r.x, r.y), backgroundcolor='magenta')\n",
    "\n",
    "    # Get faces\n",
    "    if analysis.faces:\n",
    "        # Draw a rectangle around each face\n",
    "        for face in analysis.faces:\n",
    "            r = face.face_rectangle\n",
    "            bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            draw.rectangle(bounding_box, outline='lightgreen', width=5)\n",
    "            annotation = 'Person aged approxilately {}'.format(face.age)\n",
    "            plt.annotate(annotation,(r.left, r.top), backgroundcolor='lightgreen')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # Add a second plot for addition details\n",
    "    a = fig.add_subplot(1,2,2)\n",
    "\n",
    "    # Get ratings\n",
    "    ratings = 'Ratings:\\n - Adult: {}\\n - Racy: {}\\n - Gore: {}'.format(analysis.adult.is_adult_content,\n",
    "                                                                           analysis.adult.is_racy_content,\n",
    "                                                                           analysis.adult.is_gory_content,)\n",
    "\n",
    "    # Get tags\n",
    "    tags = 'Tags:'\n",
    "    for tag in analysis.tags:\n",
    "        tags = tags + '\\n - {}'.format(tag.name)\n",
    "\n",
    "    # Print details\n",
    "\n",
    "    details = '{}\\n\\n{}'.format(ratings, tags)\n",
    "    a.text(0,0.4, details, fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d986aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure.cognitiveservices.vision.computervision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af077081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the path to an image file\n",
    "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
    "\n",
    "# Get a client for the computer vision service\n",
    "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Get a description from the computer vision service\n",
    "image_stream = open(image_path, \"rb\")\n",
    "description = computervision_client.describe_image_in_stream(image_stream)\n",
    "\n",
    "# Display image and caption (code in helper_scripts/vision.py)\n",
    "show_image_caption(image_path, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f69efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to an image file\n",
    "image_path = os.path.join('data', 'vision', 'store_cam2.jpg')\n",
    "\n",
    "# Get a description from the computer vision service\n",
    "image_stream = open(image_path, \"rb\")\n",
    "description = computervision_client.describe_image_in_stream(image_stream)\n",
    "\n",
    "# Display image and caption (code in helper_scripts/vision.py)\n",
    "show_image_caption(image_path, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ced5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to an image file\n",
    "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
    "\n",
    "# Specify the features we want to analyze\n",
    "features = ['Description', 'Tags', 'Adult', 'Objects', 'Faces']\n",
    "\n",
    "# Get an analysis from the computer vision service\n",
    "image_stream = open(image_path, \"rb\")\n",
    "analysis = computervision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
    "\n",
    "# Show the results of analysis (code in helper_scripts/vision.py)\n",
    "show_image_analysis(image_path, analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
